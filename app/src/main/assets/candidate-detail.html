<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Rafael Briggs</title>
  </head>
  <body>
    <h1>Rafael Briggs</h1>
    <img
      class="img-fluid"
      src="https://bootdey.com/img/Content/avatar/avatar1.png"
      alt=""
    />
    <p>
      In Natural Language Processing, the use of pre-trained language models has
      been shown to obtain state-of-the-art results in many downstream tasks
      such as sentiment analysis, author identification and others. In this
      work, we address the use of these methods for personality classification
      from text. Focusing on the Myers-Briggs (MBTI) personality model, we
      describe a series of experiments in which the well-known Bidirectional
      Encoder Representations from Transformers (BERT) model is fine-tuned to
      perform MBTI classification. Our main findings suggest that the current
      approach significantly outperforms well-known text classification models
      based on bag-of-words and static word embeddings alike across multiple
      evaluation scenarios, and generally outperforms previous work in the
      field.
    </p>
  </body>
</html>
